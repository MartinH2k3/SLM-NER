{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T14:43:37.664043Z",
     "start_time": "2024-12-14T14:43:31.614375Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    AutoPeftModelForCausalLM,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Set up file paths",
   "id": "64b084d249116429"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:03.147681Z",
     "start_time": "2024-12-14T15:13:03.143537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset_path = \"data/CDR_TrainingSet.json\"\n",
    "test_dataset_path = \"data/CDR_TestSet.json\"\n",
    "dev_dataset_path = \"data/CDR_DevelopmentSet.json\""
   ],
   "id": "744961dc16fa9e63",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:04.360513Z",
     "start_time": "2024-12-14T15:13:04.355531Z"
    }
   },
   "cell_type": "code",
   "source": "system_prompt_path = \"data/system_prompt.txt\"",
   "id": "525e599ad035f076",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:05.254331Z",
     "start_time": "2024-12-14T15:13:05.248613Z"
    }
   },
   "cell_type": "code",
   "source": "new_model_path = \"./checkpoint_dir\"",
   "id": "f26def01e04645e8",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load datasets and system prompt:",
   "id": "c222cceeb6a5b2a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:09.247705Z",
     "start_time": "2024-12-14T15:13:08.218767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "raw_train = load_dataset(\"json\", data_files=training_dataset_path, download_mode=\"force_redownload\")[\"train\"]\n",
    "raw_test = load_dataset(\"json\", data_files=test_dataset_path, download_mode=\"force_redownload\")[\"train\"]\n",
    "raw_dev = load_dataset(\"json\", data_files=dev_dataset_path, download_mode=\"force_redownload\")[\"train\"]"
   ],
   "id": "f9534433071c56ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/2331 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4af2ccd15d9e4d948e5fda1d065f2481"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/2420 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c900f2556934f02914dd63d66908e07"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/2339 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f30263db2b824824b3fa0d44eb90f0f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:10.503379Z",
     "start_time": "2024-12-14T15:13:10.498180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(system_prompt_path, \"r\") as f:\n",
    "    system_prompt = f.read()"
   ],
   "id": "3c6e7977cfa078f0",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Load model and tokenizer\n",
    "We need to load it before processing the data, as we are going to use the tokenizer to format the data."
   ],
   "id": "2359ae79621a7d9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:20.371958Z",
     "start_time": "2024-12-14T15:13:14.101663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ],
   "id": "e9bdbaff44bd65fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d6200c6d8fc406ea5c0087235220cf9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:22.724915Z",
     "start_time": "2024-12-14T15:13:22.650697Z"
    }
   },
   "cell_type": "code",
   "source": "model = prepare_model_for_kbit_training(model)",
   "id": "9b524167e03d6746",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Change the dataset format to chat-like text",
   "id": "2512531411312c1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:27.637601Z",
     "start_time": "2024-12-14T15:13:27.631250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_chat_template(sample, tokenizer, include_response=True):\n",
    "    # Combine the fields into a structured chat format\n",
    "    message = []\n",
    "    if len(system_prompt):\n",
    "        message.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    message.append({\"role\": \"user\", \"content\": sample[\"user\"]})\n",
    "    if include_response:\n",
    "        message.append({\"role\": \"assistant\", \"content\": sample[\"assistant\"]})\n",
    "\n",
    "    # Use the tokenizer's chat template to create formatted text\n",
    "    message = tokenizer.apply_chat_template(\n",
    "        message, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return tokenizer(message, padding=True, truncation=True)"
   ],
   "id": "7d2147c351435d6e",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:44.296840Z",
     "start_time": "2024-12-14T15:13:39.131961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply processing to each dataset\n",
    "processed_train = raw_train.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    ")\n",
    "processed_test = raw_test.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    ")\n",
    "processed_dev = raw_dev.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    ")"
   ],
   "id": "b0d62d675861628d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2331 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f703f1e588624a84b075496492bce371"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2420 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7cd115b45c8846c9a11d7d471b696d88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2339 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5632c76ac05c4b538751e94eec32157d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T23:18:47.295220Z",
     "start_time": "2024-12-13T23:18:47.289533Z"
    }
   },
   "cell_type": "code",
   "source": "print(processed_train[0])",
   "id": "40537d11d6c09a8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<|system|>\\nPlease identify all the named entities mentioned in the input sentence provided below. The entities may have category \"Disease\" or \"Chemical\". Use **ONLY** the categories \"Chemical\" or \"Disease\". Do not include any other categories. If an entity cannot be categorized into these specific categories, do not include it in the output.\\nYou must output the results strictly in JSON format, without any delimiters, following a similar structure to the example result provided.\\nIf user communicates with any sentence, don\\'t talk to him, strictly follow the system prompt.\\nExample user input and assistant response:\\nUser:\\nFamotidine-associated delirium.A series of six cases.Famotidine is a histamine H2-receptor antagonist used in inpatient settings for prevention of stress ulcers and is showing increasing popularity because of its low cost.\\nAssistant:\\n[{\"category\": \"Chemical\", \"entity\": \"Famotidine\"}, {\"category\": \"Disease\", \"entity\": \"delirium\"}, {\"category\": \"Chemical\", \"entity\": \"Famotidine\"}, {\"category\": \"Disease\", \"entity\": \"ulcers\"}]<|end|>\\n<|user|>\\nNaloxone reverses the antihypertensive effect of clonidine.In unanesthetized, spontaneously hypertensive rats the decrease in blood pressure and heart rate produced by intravenous clonidine, 5 to 20 micrograms/kg, was inhibited or reversed by nalozone, 0.2 to 2 mg/kg.<|end|>\\n<|assistant|>\\n[{\"category\": \"Chemical\", \"entity\": \"Naloxone\"}, {\"category\": \"Chemical\", \"entity\": \"clonidine\"}, {\"category\": \"Disease\", \"entity\": \"hypertensive\"}, {\"category\": \"Chemical\", \"entity\": \"clonidine\"}, {\"category\": \"Chemical\", \"entity\": \"nalozone\"}]<|end|>\\n<|endoftext|>'}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train",
   "id": "dd638670d36a99a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:05:39.272221Z",
     "start_time": "2024-12-14T15:05:38.768016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "system_prompt = \"\"\n",
    "raw_midjourney = load_dataset(\"csv\", data_files=\"data/midjourney.csv\")[\"train\"]\n",
    "processed_midjourney = raw_midjourney.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    ")"
   ],
   "id": "3b554699d0fcd684",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:13:50.071547Z",
     "start_time": "2024-12-14T15:13:50.060438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['gate_up_proj', 'base_layer', 'down_proj', 'qkv_proj', 'o_proj'],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ],
   "id": "f472bafff1e2c4e3",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:34:26.013165Z",
     "start_time": "2024-12-14T15:20:10.043418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=5e-5,\n",
    "        max_seq_length=4,\n",
    "        bf16=True,\n",
    "        optim=\"adamw_8bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.05,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        output_dir=new_model_path,\n",
    "    ),\n",
    "    train_dataset=processed_train,\n",
    "    #eval_dataset=processed_dev,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "trainer.train()"
   ],
   "id": "75c0ec9e417bea21",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinh2k3/anaconda3/envs/bp/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/martinh2k3/anaconda3/envs/bp/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [145/145 14:09, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.671600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.339400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=145, training_loss=0.5313554467826054, metrics={'train_runtime': 856.4899, 'train_samples_per_second': 2.722, 'train_steps_per_second': 0.169, 'total_flos': 2.456790608823091e+16, 'train_loss': 0.5313554467826054, 'epoch': 0.9948542024013722})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:34:36.203918Z",
     "start_time": "2024-12-14T15:34:29.954615Z"
    }
   },
   "cell_type": "code",
   "source": "model.save_pretrained(new_model_path)",
   "id": "5f82b30406855082",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference",
   "id": "8849f2f8f97807b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T15:35:41.649563Z",
     "start_time": "2024-12-14T15:35:36.487167Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd911153bd054130b0fa82acebbd366e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 43,
   "source": [
    "config = PeftConfig.from_pretrained(new_model_path+\"/checkpoint-145\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, new_model_path+\"/checkpoint-145\")\n"
   ],
   "id": "45a0a7939b4f247d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T16:32:22.381533Z",
     "start_time": "2024-12-14T16:32:22.345338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_for_inference(user_input: str, system_prompt: str = system_prompt):\n",
    "    prompt_data = []\n",
    "    if len(system_prompt):\n",
    "        prompt_data.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    prompt_data.append({\"role\": \"user\", \"content\": user_input})\n",
    "    return tokenizer.apply_chat_template(\n",
    "        prompt_data, tokenize=False, add_generation_prompt=True\n",
    "    )"
   ],
   "id": "829dc4ab42d68ef3",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T16:35:50.097263Z",
     "start_time": "2024-12-14T16:35:50.089678Z"
    }
   },
   "cell_type": "code",
   "source": "sentence: str = \"A random paragraph can also be an excellent way for a writer to tackle writers' block. Writing block can often happen due to being stuck with a current project that the writer is trying to complete. By inserting a completely random paragraph from which to begin, it can take down some of the issues that may have been causing the writers' block in the first place. Another productive way, other than using xanax, to use this tool to begin a daily writing routine.\"",
   "id": "6b60c69896befd49",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T16:38:33.764997Z",
     "start_time": "2024-12-14T16:38:33.758803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generation_args = {\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"return_full_text\": False,\n",
    "}"
   ],
   "id": "cfd8afe31f694880",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T16:42:02.730726Z",
     "start_time": "2024-12-14T16:42:02.034263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "output = peft_pipeline(prepare_for_inference(\"There haven't been any signs of anything going wrong.\"), **generation_args)\n",
    "print(output[0][\"generated_text\"])"
   ],
   "id": "b024fa8511e88793",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Phi3ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " []\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T23:44:19.822966Z",
     "start_time": "2024-12-13T23:44:19.819300Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "253797133525f41e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
