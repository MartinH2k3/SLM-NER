{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T17:39:13.178572Z",
     "start_time": "2024-12-07T17:39:07.425600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "import peft\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "import wandb"
   ],
   "id": "77727fc6953a6433",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Set up paths to datasets",
   "id": "2857eeff2db988d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T17:43:04.847193Z",
     "start_time": "2024-12-07T17:43:04.843705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_dataset_path = \"data/CDR_TrainingSet.json\"\n",
    "test_dataset_path = \"data/CDR_TestSet.json\"\n",
    "dev_dataset_path = \"data/CDR_DevelopmentSet.json\""
   ],
   "id": "29d1c325569566c4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T18:47:01.713258Z",
     "start_time": "2024-12-07T18:47:01.708926Z"
    }
   },
   "cell_type": "code",
   "source": "system_prompt_path = \"data/system_prompt.txt\"",
   "id": "831f400ae2dbbc2e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "4bfcb4ed-a220-4abe-bfd3-c360addd95ae",
   "metadata": {},
   "source": "#### Load datasets and system prompt:"
  },
  {
   "cell_type": "code",
   "id": "0ed436d0-22f3-4669-8ede-d21bbe979823",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T17:43:07.563106Z",
     "start_time": "2024-12-07T17:43:06.562247Z"
    }
   },
   "source": [
    "raw_train = datasets.load_dataset(\"json\", data_files=training_dataset_path, download_mode=\"force_redownload\")[\"train\"]\n",
    "raw_test = datasets.load_dataset(\"json\", data_files=test_dataset_path, download_mode=\"force_redownload\")[\"train\"]\n",
    "raw_dev = datasets.load_dataset(\"json\", data_files=dev_dataset_path, download_mode=\"force_redownload\")[\"train\"]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcfa492231ba4c6785810c00af483027"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bec08e9f7b47424e99bd4553fc5a650f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c96a3727e9c84bdbacbd7c494386f8e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T18:47:03.730051Z",
     "start_time": "2024-12-07T18:47:03.724660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(system_prompt_path, \"r\") as f:\n",
    "    system_prompt = f.read()"
   ],
   "id": "5190ffb383b337bb",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Load model and tokenizer\n",
    "We need to load it before processing the data, as we are going to use the tokenizer to format the data."
   ],
   "id": "95d25af68d770079"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T17:43:12.274376Z",
     "start_time": "2024-12-07T17:43:11.063179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 2048\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'"
   ],
   "id": "7a80cf93-b1cd-48be-ad64-8ca12bb15a61",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19d1e6102359426f8c90fb2fd7649564"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Change the dataset format to chat-like text",
   "id": "69fe7bd3-193d-4045-abbd-cfef805e3a54"
  },
  {
   "cell_type": "code",
   "id": "9a098ab7-aa0a-4d8b-bce0-56a4f7b5af3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T17:43:15.554234Z",
     "start_time": "2024-12-07T17:43:14.927988Z"
    }
   },
   "source": [
    "def apply_chat_template(example, tokenizer):\n",
    "    \"\"\"\n",
    "    Convert the system, input, and output fields into a formatted chat-like text.\n",
    "    \"\"\"\n",
    "    # Combine the fields into a structured chat format\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": example[\"user\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"assistant\"]}\n",
    "    ]\n",
    "    # Use the tokenizer's chat template to create formatted text\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return example\n",
    "\n",
    "column_names = list(raw_train.features)\n",
    "\n",
    "# Apply processing to each dataset\n",
    "processed_train = raw_train.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    remove_columns=column_names,\n",
    ")\n",
    "processed_test = raw_test.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    remove_columns=column_names,\n",
    ")\n",
    "processed_dev = raw_dev.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    remove_columns=column_names,\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2331 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b35d5fdc15ab49e9b5721eaabc13b7b9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2420 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2abbc8af669429081f3b793d61c07e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2339 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb4638e6db12413b924714b113cd3575"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "a0f7ad9d-f6fd-4eaf-b62a-fa92d17232e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T17:48:07.231762Z",
     "start_time": "2024-12-07T17:48:07.223644Z"
    }
   },
   "source": [
    "tokens = tokenizer(processed_train[89][\"text\"])\n",
    "tokens = [tokenizer.decode(token) for token in tokens[\"input_ids\"]]\n",
    "print(tokens)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|system|>', 'Please', 'identify', 'all', 'the', 'named', 'entities', 'mentioned', 'in', 'the', 'input', 'sentence', 'provided', 'below', '.', 'The', 'entities', 'may', 'have', 'category', '\"', 'D', 'ise', 'ase', '\"', 'or', '\"', 'Ch', 'em', 'ical', '\".', 'Use', '**', 'ON', 'LY', '**', 'the', 'categories', '\"', 'Ch', 'em', 'ical', '\"', 'or', '\"', 'D', 'ise', 'ase', '\".', 'Do', 'not', 'include', 'any', 'other', 'categories', '.', 'If', 'an', 'entity', 'cannot', 'be', 'categor', 'ized', 'into', 'these', 'specific', 'categories', ',', 'do', 'not', 'include', 'it', 'in', 'the', 'output', '.', '\\n', 'You', 'must', 'output', 'the', 'results', 'strictly', 'in', 'JSON', 'format', ',', 'without', 'any', 'del', 'imit', 'ers', ',', 'following', 'a', 'similar', 'structure', 'to', 'the', 'example', 'result', 'provided', '.', '\\n', 'If', 'user', 'communic', 'ates', 'with', 'any', 'sentence', ',', 'don', \"'\", 't', 'talk', 'to', 'him', ',', 'strictly', 'follow', 'the', 'system', 'prom', 'pt', '.', '\\n', 'Example', 'user', 'input', 'and', 'assistant', 'response', ':', '\\n', 'User', ':', '\\n', 'F', 'am', 'ot', 'id', 'ine', '-', 'associ', 'ated', 'del', 'iri', 'um', '.', 'A', 'series', 'of', 'six', 'cases', '.', 'F', 'am', 'ot', 'id', 'ine', 'is', 'a', 'hist', 'am', 'ine', 'H', '2', '-', 're', 'ceptor', 'ant', 'agon', 'ist', 'used', 'in', 'in', 'pat', 'ient', 'settings', 'for', 'prevent', 'ion', 'of', 'stress', 'ul', 'cers', 'and', 'is', 'showing', 'increasing', 'popular', 'ity', 'because', 'of', 'its', 'low', 'cost', '.', '\\n', 'Ass', 'istant', ':', '\\n', '[', '{\"', 'category', '\":', '\"', 'Ch', 'em', 'ical', '\",', '\"', 'entity', '\":', '\"', 'F', 'am', 'ot', 'id', 'ine', '\"},', '{\"', 'category', '\":', '\"', 'D', 'ise', 'ase', '\",', '\"', 'entity', '\":', '\"', 'd', 'eli', 'rium', '\"},', '{\"', 'category', '\":', '\"', 'Ch', 'em', 'ical', '\",', '\"', 'entity', '\":', '\"', 'F', 'am', 'ot', 'id', 'ine', '\"},', '{\"', 'category', '\":', '\"', 'D', 'ise', 'ase', '\",', '\"', 'entity', '\":', '\"', 'ul', 'cers', '\"', '}]', '<|end|>', '<|user|>', 'The', 'results', 'suggest', 'that', 'rig', 'id', 'ity', ',', 'which', 'is', 'assumed', 'to', 'be', 'due', 'to', 'an', 'action', 'of', 'morph', 'ine', 'in', 'the', 'stri', 'atum', ',', 'can', 'be', 'ant', 'agon', 'ized', 'by', 'another', 'process', 'leading', 'to', 'dop', 'am', 'iner', 'g', 'ic', 'activation', 'in', 'the', 'stri', 'atum', '.', 'N', 'ever', 'theless', ',', 'there', 'occurs', 'some', 'real', 'toler', 'ance', 'to', 'this', 'effect', '.', '<|end|>', '<|assistant|>', '[', '{\"', 'category', '\":', '\"', 'D', 'ise', 'ase', '\",', '\"', 'entity', '\":', '\"', 'rig', 'id', 'ity', '\"},', '{\"', 'category', '\":', '\"', 'Ch', 'em', 'ical', '\",', '\"', 'entity', '\":', '\"', 'm', 'orph', 'ine', '\"', '}]', '<|end|>', '<|endoftext|>']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:36:58.951035Z",
     "start_time": "2024-12-03T12:36:58.945902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_config = {\n",
    "    \"fp16\": True,\n",
    "    \"do_eval\": False,\n",
    "    # \"evaluation_strategy\": \"steps\",\n",
    "    # \"eval_steps\": 20,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"logging_steps\": 5,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    # \"max_steps\": 30,\n",
    "    \"output_dir\": \"./checkpoint_dir\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    # \"per_device_eval_batch_size\": 10,\n",
    "    \"per_device_train_batch_size\": 2,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 20,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_ratio\": 0.05,\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    }"
   ],
   "id": "3543a2f3-8e26-4ac8-9e5b-1859e209f115",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:37:03.247648Z",
     "start_time": "2024-12-03T12:37:03.202747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": \"all-linear\",\n",
    "    \"modules_to_save\": None,\n",
    "}\n",
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)"
   ],
   "id": "2d841e1b1ae6a802",
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "id": "c6b09e33-61b9-4527-974a-3e7b88b185e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:35:44.681481Z",
     "start_time": "2024-12-03T12:37:05.225546Z"
    }
   },
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    peft_config=peft_conf,\n",
    "    train_dataset=processed_train,\n",
    "    eval_dataset=processed_dev,\n",
    "    max_seq_length=4,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "trainer.save_state()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinh2k3/anaconda3/envs/bp/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/martinh2k3/anaconda3/envs/bp/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/martinh2k3/anaconda3/envs/bp/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='291' max='291' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [291/291 57:32, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.603000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.922700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     0.9983\n",
      "  total_flos               =   195013GF\n",
      "  train_loss               =     0.3625\n",
      "  train_runtime            = 0:58:11.09\n",
      "  train_samples_per_second =      0.668\n",
      "  train_steps_per_second   =      0.083\n",
      "***** eval metrics *****\n",
      "  epoch                    =     0.9983\n",
      "  total_flos               =   195013GF\n",
      "  train_loss               =     0.3625\n",
      "  train_runtime            = 0:58:11.09\n",
      "  train_samples_per_second =      0.668\n",
      "  train_steps_per_second   =      0.083\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T16:22:31.871686Z",
     "start_time": "2024-11-28T16:22:20.667233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_model\")"
   ],
   "id": "f48073575a0facbd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model/tokenizer_config.json',\n",
       " './trained_model/special_tokens_map.json',\n",
       " './trained_model/tokenizer.model',\n",
       " './trained_model/added_tokens.json',\n",
       " './trained_model/tokenizer.json')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "id": "b9dded35-6eeb-4189-97df-7d3acf86e94a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T11:47:23.571258Z",
     "start_time": "2024-12-03T11:47:23.566227Z"
    }
   },
   "source": [
    "def prepare_for_inference(user_input: str, system_prompt: str = system_prompt):\n",
    "    prompt_data = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "    return  tokenizer.apply_chat_template(\n",
    "        prompt_data, tokenize=False, add_generation_prompt=\"<|assistant|>\" \n",
    "    )\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "c4b36dd6-b711-4784-90d3-581fea1125b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:30:18.341533Z",
     "start_time": "2024-12-03T12:30:18.332107Z"
    }
   },
   "source": [
    "processed_input = prepare_for_inference(\"A random paragraph can also be an excellent way for a writer to tackle writers' block. Writing block can often happen due to being stuck with a current project that the writer is trying to complete. By inserting a completely random paragraph from which to begin, it can take down some of the issues that may have been causing the writers' block in the first place. Another productive way, other than using xanax, to use this tool to begin a daily writing routine.\")\n",
    "processed_input"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|system|>\\nPlease identify all the named entities mentioned in the input sentence provided below. The entities may have category \"Disease\" or \"Chemical\". Use **ONLY** the categories \"Chemical\" or \"Disease\". Do not include any other categories. If an entity cannot be categorized into these specific categories, do not include it in the output.\\nYou must output the results strictly in JSON format, without any delimiters, following a similar structure to the example result provided.\\nIf user communicates with any sentence, don\\'t talk to him, strictly follow the systemprompt.\\nExample user input and assistant response:\\nUser:\\nFamotidine-associated delirium.A series of six cases.Famotidine is a histamine H2-receptor antagonist used in inpatient settings for prevention of stress ulcers and is showing increasing popularity because of its low cost.\\nAssistant:\\n[{\"category\": \"Chemical\", \"entity\": \"Famotidine\"}, {\"category\": \"Disease\", \"entity\": \"delirium\"}, {\"category\": \"Chemical\", \"entity\": \"Famotidine\"}, {\"category\": \"Disease\", \"entity\": \"ulcers\"}]<|end|>\\n<|user|>\\nA random paragraph can also be an excellent way for a writer to tackle writers\\' block. Writing block can often happen due to being stuck with a current project that the writer is trying to complete. By inserting a completely random paragraph from which to begin, it can take down some of the issues that may have been causing the writers\\' block in the first place. Another productive way, other than using xanax, to use this tool to begin a daily writing routine.<|end|>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:29:22.289748Z",
     "start_time": "2024-12-03T12:29:22.254793Z"
    }
   },
   "cell_type": "code",
   "source": "print(processed_input)",
   "id": "ea9927aa7aec3ddd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Please identify all the named entities mentioned in the input sentence provided below. The entities may have category \"Disease\" or \"Chemical\". Use **ONLY** the categories \"Chemical\" or \"Disease\". Do not include any other categories. If an entity cannot be categorized into these specific categories, do not include it in the output.\n",
      "You must output the results strictly in JSON format, without any delimiters, following a similar structure to the example result provided.\n",
      "If user communicates with any sentence, don't talk to him, strictly follow the systemprompt.\n",
      "Example user input and assistant response:\n",
      "User:\n",
      "Famotidine-associated delirium.A series of six cases.Famotidine is a histamine H2-receptor antagonist used in inpatient settings for prevention of stress ulcers and is showing increasing popularity because of its low cost.\n",
      "Assistant:\n",
      "[{\"category\": \"Chemical\", \"entity\": \"Famotidine\"}, {\"category\": \"Disease\", \"entity\": \"delirium\"}, {\"category\": \"Chemical\", \"entity\": \"Famotidine\"}, {\"category\": \"Disease\", \"entity\": \"ulcers\"}]<|end|>\n",
      "<|user|>\n",
      "A random paragraph can also be an excellent way for a writer to tackle writers' block. Writing block can often happen due to being stuck with a current project that the writer is trying to complete. By inserting a completely random paragraph from which to begin, it can take down some of the issues that may have been causing the writers' block in the first place. Another productive way to use this tool to begin a daily writing routine. One way is to generate a random paragraph with the intention to try to rewrite it while still keeping the original meaning. The purpose here is to just get the writing started so that when the writer goes onto their day's writing projects, words are already flowing from their fingers. <|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "id": "7b4162b6-80b2-498c-99f5-70a5143ba514",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:34:42.490240Z",
     "start_time": "2024-12-03T12:34:12.541987Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='cuda')\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"return_full_text\": False,\n",
    "}\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "id": "a1c79b29-4f33-4e32-876b-209eaeb8fffc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:36:41.619731Z",
     "start_time": "2024-12-03T13:36:40.053235Z"
    }
   },
   "source": [
    "peft_model = PeftModel.from_pretrained(model, \"checkpoint_dir/checkpoint-291\", adapter_name=\"idk\")\n",
    "peft_model.load_adapter(\"checkpoint_dir/checkpoint-291\", adapter_name=\"tfisthis\")\n",
    "peft_model.eval()\n",
    "peft_pipeline = pipeline(\"text-generation\", model=peft_model, tokenizer=tokenizer, device='cuda')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Phi3ForCausalLM'].\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:20:16.924699Z",
     "start_time": "2024-12-03T12:20:16.901469Z"
    }
   },
   "cell_type": "code",
   "source": "peft_model.eval()",
   "id": "34b0ffc8fc45d171",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3Attention(\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                  (idk): Dropout(p=0.05, inplace=False)\n",
       "                  (tfisthis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                  (idk): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                  (tfisthis): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                  (idk): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                  (tfisthis): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                  (idk): Dropout(p=0.05, inplace=False)\n",
       "                  (tfisthis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                  (idk): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                  (tfisthis): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                  (idk): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                  (tfisthis): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Phi3RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                  (idk): Dropout(p=0.05, inplace=False)\n",
       "                  (tfisthis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                  (idk): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                  (tfisthis): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                  (idk): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                  (tfisthis): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                  (idk): Dropout(p=0.05, inplace=False)\n",
       "                  (tfisthis): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                  (idk): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                  (tfisthis): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                  (idk): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                  (tfisthis): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm()\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T12:36:11.139374Z",
     "start_time": "2024-12-03T12:36:03.140162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = nlp(processed_input, **generation_args)\n",
    "output[0][\"generated_text\"]"
   ],
   "id": "dcdd7892556cba47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [{\"category\": \"Chemical\", \"entity\": \"xanax\"}, {\"category\": \"Disease\", \"entity\": \"writers\\' block\"}]'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:37:33.217025Z",
     "start_time": "2024-12-03T13:36:46.274783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = peft_pipeline(processed_input, **generation_args)\n",
    "output[0][\"generated_text\"]"
   ],
   "id": "d5ed49a3360810d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [{\"category\": \"Chemical\", \"entity\": \"xanax\"}, {\"category\": \"Disease\", \"entity\": \"writers\\' block\"}]'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:46:36.033375Z",
     "start_time": "2024-12-03T13:46:35.823912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_datset_for_inference(dataset, user_column=\"user\"):\n",
    "    processed_data = []\n",
    "    for item in dataset:\n",
    "        processed_input = prepare_for_inference(item[user_column])\n",
    "        processed_data.append(processed_input)\n",
    "    return processed_data\n",
    "\n",
    "test = prepare_datset_for_inference(raw_test)[:30]"
   ],
   "id": "3c29fa819d6ae248",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:50:16.470898Z",
     "start_time": "2024-12-03T13:46:41.977596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output1 = peft_pipeline(test, **generation_args)\n",
    "output2 = nlp(test, **generation_args)"
   ],
   "id": "ace9de6a09d027c1",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:50:16.548523Z",
     "start_time": "2024-12-03T14:50:16.523895Z"
    }
   },
   "cell_type": "code",
   "source": "output1 == output2",
   "id": "d1b2467f0dadaa34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:54:51.984649Z",
     "start_time": "2024-12-03T14:54:51.979731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in range(30):\n",
    "    if output1[i] != output2[i]:\n",
    "        print(i)\n",
    "        print(output1[i][0][\"generated_text\"])\n",
    "        print(output2[i][0][\"generated_text\"])"
   ],
   "id": "212bc1691348fe7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      " [{\"category\": \"Disease\", \"entity\": \"Scleroderma renal crisis\"}, {\"category\": \"Disease\", \"entity\": \"systemic sclerosis\"}, {\"category\": \"Disease\", \"entity\": \"renal replacement therapy\"}, {\"category\": \"Disease\", \"entity\": \"Scleroderma renal crisis\"}, {\"category\": \"Disease\", \"entity\": \"systemic sclerosis\"}, {\"category\": \"Chemical\", \"entity\": \"corticosteroid\"}, {\"category\": \"Disease\", \"entity\": \"Scleroderma renal crisis\"}, {\"category\": \"Disease\", \"entity\": \"systemic sclerosis\"}, {\"category\": \"Disease\", \"entity\": \"renal replacement therapy\"}]\n",
      " [{\"category\": \"Disease\", \"entity\": \"Scleroderma renal crisis\"}, {\"category\": \"Disease\", \"entity\": \"systemic sclerosis\"}, {\"category\": \"Disease\", \"entity\": \"renal replacement therapy\"}, {\"category\": \"Disease\", \"entity\": \"Scleroderma renal crisis\"}, {\"category\": \"Disease\", \"entity\": \"systemic sclerosis\"}, {\"category\": \"Chemical\", \"entity\": \"corticosteroid\"}, {\"category\": \"Disease\", \"entity\": \"Scleroderma renal crisis\"}, {\"category\": \"Disease\", \"entity\": \"systemic sclerosis\"}]\n",
      "6\n",
      " [{\"category\": \"Disease\", \"entity\": \"thrombotic microangiopathy\"}, {\"category\": \"Disease\", \"entity\": \"Systemic sclerosis (SSc)\"}, {\"category\": \"Disease\", \"entity\": \"Systemic rheumatic colitis (SRC)\"}, {\"category\": \"Chemical\", \"entity\": \"cyclosporine\"}, {\"category\": \"Chemical\", \"entity\": \"tacrolimus\"}, {\"category\": \"Chemical\", \"entity\": \"corticosteroids\"}]\n",
      " [{\"category\": \"Disease\", \"entity\": \"thrombotic microangiopathy\"}, {\"category\": \"Disease\", \"entity\": \"Systemic sclerosis (SSc)\"}, {\"category\": \"Disease\", \"entity\": \"Systemic sclerosis-related renal crisis (SRC)\"}, {\"category\": \"Chemical\", \"entity\": \"cyclosporine\"}, {\"category\": \"Chemical\", \"entity\": \"tacrolimus\"}, {\"category\": \"Chemical\", \"entity\": \"corticosteroids\"}]\n",
      "18\n",
      " [{\"category\": \"Chemical\", \"entity\": \"antidyskinetic effect\"}, {\"category\": \"Chemical\", \"entity\": \"sensorimotor plasticity\"}, {\"category\": \"Chemical\", \"entity\": \"M1\"}]\n",
      " [{\"category\": \"Chemical\", \"entity\": \"antidyskinetic effect\"}, {\"category\": \"Disease\", \"entity\": \"sensorimotor plasticity\"}, {\"category\": \"Chemical\", \"entity\": \"M1\"}]\n",
      "20\n",
      " [{\"category\": \"Chemical\", \"entity\": \"cyclophosphamide\"}, {\"category\": \"Disease\", \"entity\": \"cystitis\"}, {\"category\": \"Chemical\", \"entity\": \"P2X3 receptor antagonists\"}, {\"category\": \"Chemical\", \"entity\": \"NK1 receptor antagonists\"}, {\"category\": \"Chemical\", \"entity\": \"cyclophosphamide\"}, {\"category\": \"Chemical\", \"entity\": \"P2X3 receptor antagonists\"}, {\"category\": \"Chemical\", \"entity\": \"NK1 receptor antagonists\"}]\n",
      " [{\"category\": \"Chemical\", \"entity\": \"cyclophosphamide\"}, {\"category\": \"Disease\", \"entity\": \"cystitis\"}, {\"category\": \"Chemical\", \"entity\": \"P2X3 receptor antagonists\"}, {\"category\": \"Chemical\", \"entity\": \"NK1 receptor antagonists\"}]\n",
      "27\n",
      " [{\"category\": \"Disease\", \"entity\": \"drug-induced hepatic injury\"}, {\"category\": \"Chemical\", \"entity\": \"clopidogrel\"}, {\"category\": \"Chemical\", \"entity\": \"Bortezomib\"}, {\"category\": \"Chemical\", \"entity\": \"dexamethasone\"}, {\"category\": \"Disease\", \"entity\": \"multiple myeloma\"}, {\"category\": \"Disease\", \"entity\": \"relapsed/refractory multiple myeloma\"}, {\"category\": \"Chemical\", \"entity\": \"salvage therapy\"}]\n",
      " [{\"category\": \"Chemical\", \"entity\": \"clopidogrel\"}, {\"category\": \"Chemical\", \"entity\": \"Bortezomib\"}, {\"category\": \"Chemical\", \"entity\": \"dexamethasone\"}, {\"category\": \"Disease\", \"entity\": \"multiple myeloma\"}, {\"category\": \"Disease\", \"entity\": \"relapsed/refractory multiple myeloma\"}, {\"category\": \"Disease\", \"entity\": \"hepatic injury\"}]\n",
      "29\n",
      " [{\"category\": \"Chemical\", \"entity\": \"bort\"}, {\"category\": \"Chemical\", \"entity\": \"dex\"}, {\"category\": \"Disease\", \"entity\": \"multiple myeloma\"}, {\"category\": \"Disease\", \"entity\": \"relapsed/refractory MM\"}, {\"category\": \"Chemical\", \"entity\": \"autologous stem cell transplantation\"}, {\"category\": \"Chemical\", \"entity\": \"conventional chemotherapy\"}, {\"category\": \"Chemical\", \"entity\": \"bort\"}, {\"category\": \"Chemical\", \"entity\": \"dex\"}]\n",
      " [{\"category\": \"Chemical\", \"entity\": \"bort\"}, {\"category\": \"Chemical\", \"entity\": \"dex\"}, {\"category\": \"Disease\", \"entity\": \"multiple myeloma\"}, {\"category\": \"Disease\", \"entity\": \"relapsed/refractory MM\"}, {\"category\": \"Disease\", \"entity\": \"autologous stem cell transplantation\"}, {\"category\": \"Disease\", \"entity\": \"conventional chemotherapy\"}, {\"category\": \"Chemical\", \"entity\": \"bort\"}, {\"category\": \"Chemical\", \"entity\": \"dex\"}]\n"
     ]
    }
   ],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f6f52008a01828f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
